# Global LLM configuration
# [llm]
# model = "google_gemma-3-12b-it-Q8_0.gguf"        # The LLM model to use
# base_url = "http://localhost:8080/v1"            # API endpoint URL
# api_key = "fake_key"                             # Your API key
# max_tokens = 128000                              # Maximum number of tokens in the response
# temperature = 0.7                                # Controls randomness
# Global LLM configuration
[llm]
#model = "google_gemma-3-12b-it-Q8_0.gguf"
model = "ollama"
base_url = "http://localhost:8080/v1"
api_type = "ollama"
api_key = "fake_key"  # Replace with your actual API key
max_tokens = 128000
temperature = 0.7
