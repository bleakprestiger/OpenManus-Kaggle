# Global LLM configuration
[llm]
model = "google_gemma-3-12b-it-Q8_0.gguf"        # The LLM model to use
base_url = "http://localhost:8080/v1"            # API endpoint URL
api_key = "fake_key"                             # Your API key
max_tokens = 128000                              # Maximum number of tokens in the response
temperature = 0.7                                # Controls randomness
